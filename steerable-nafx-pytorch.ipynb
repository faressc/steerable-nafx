{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNoMnmcyG0GC"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Steerable discovery of neural audio effects\n",
        "\n",
        "  [Christian J. Steinmetz](https://www.christiansteinmetz.com/)  and  [Joshua D. Reiss](http://www.eecs.qmul.ac.uk/~josh/)\n",
        "\n",
        "\n",
        "[Code](https://github.com/csteinmetz1/steerable-nafx) • [Paper](https://arxiv.org/abs/2112.02926) • [Demo](https://csteinmetz1.github.io/steerable-nafx)\t• [Slides]()\n",
        "\n",
        "<img src=\"https://csteinmetz1.github.io/steerable-nafx/assets/steerable-headline.svg\">\n",
        "\n",
        "</div>\n",
        "\n",
        "## Abtract\n",
        "Applications of deep learning for audio effects often focus on modeling analog effects or learning to control effects to emulate a trained audio engineer. \n",
        "However, deep learning approaches also have the potential to expand creativity through neural audio effects that enable new sound transformations. \n",
        "While recent work demonstrated that neural networks with random weights produce compelling audio effects, control of these effects is limited and unintuitive.\n",
        "To address this, we introduce a method for the steerable discovery of neural audio effects.\n",
        "This method enables the design of effects using example recordings provided by the user. \n",
        "We demonstrate how this method produces an effect similar to the target effect, along with interesting inaccuracies, while also providing perceptually relevant controls.\n",
        "\n",
        "\n",
        "\\* *Accepted to NeurIPS 2021 Workshop on Machine Learning for Creativity and Design*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6x_gvWLIC8c"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchinfo\n",
        "import onnx\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choose computation device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = 'model_0'\n",
        "\n",
        "\n",
        "if not os.path.exists('models/'+name):\n",
        "    os.makedirs('models/'+name)\n",
        "else:\n",
        "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
        "    exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWjFxVtfMtq8"
      },
      "outputs": [],
      "source": [
        "class TCNBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dilation, activation=True):\n",
        "    super().__init__()\n",
        "    self.conv = torch.nn.Conv1d(\n",
        "        in_channels, \n",
        "        out_channels, \n",
        "        kernel_size, \n",
        "        dilation=dilation, \n",
        "        padding=0, #((kernel_size-1)//2)*dilation,\n",
        "        bias=True)\n",
        "    torch.nn.init.xavier_uniform_(self.conv.weight) # default is kaiming_uniform_(self.res.weight a=math.sqrt(5)) this equals U(−k,k), where k=groups/(Cin∗kernel_size) (cf. source code), but keras uses GlorotUniform which is xavier_uniform_; for timbral fx kaiming_uniform_ with! a=math.sqrt(5) seems to work better\n",
        "    torch.nn.init.zeros_(self.conv.bias) # also default is kaiming_uniform_(self.res.bias, a=math.sqrt(5)), see source code for implementation with small inchannels, but keras uses Zeros for bias\n",
        "    if activation:\n",
        "      # self.act = torch.nn.Tanh()\n",
        "      self.act = torch.nn.PReLU()\n",
        "    # this is the residual connection with 1x1 conv to match the channels and mix the desired amount of residual in\n",
        "    self.res = torch.nn.Conv1d(in_channels, out_channels, 1, bias=False)\n",
        "    torch.nn.init.xavier_uniform_(self.res.weight)\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation = dilation\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_in = x\n",
        "    x = self.conv(x)\n",
        "    if hasattr(self, \"act\"):\n",
        "      x = self.act(x)\n",
        "    x_res = self.res(x_in)\n",
        "    x_res = x_res[..., (self.kernel_size-1)*self.dilation:]\n",
        "    x = x + x_res\n",
        "    return x\n",
        "\n",
        "class TCN(torch.nn.Module):\n",
        "  def __init__(self, n_inputs=1, n_outputs=1, n_blocks=10, kernel_size=13, n_channels=64, dilation_growth=4):\n",
        "    super().__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.n_channels = n_channels\n",
        "    self.dilation_growth = dilation_growth\n",
        "    self.n_blocks = n_blocks\n",
        "    self.stack_size = n_blocks\n",
        "\n",
        "    self.blocks = torch.nn.ModuleList()\n",
        "    for n in range(n_blocks):\n",
        "      if n == 0:\n",
        "        in_ch = n_inputs\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      elif (n+1) == n_blocks:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_outputs\n",
        "        act = True\n",
        "      else:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      \n",
        "      dilation = dilation_growth ** n\n",
        "      self.blocks.append(TCNBlock(in_ch, out_ch, kernel_size, dilation, activation=act))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    return x\n",
        "  \n",
        "  def compute_receptive_field(self):\n",
        "    \"\"\"Compute the receptive field in samples.\"\"\"\n",
        "    rf = self.kernel_size\n",
        "    for n in range(1, self.n_blocks):\n",
        "        dilation = self.dilation_growth ** (n % self.stack_size)\n",
        "        rf = rf + ((self.kernel_size - 1) * dilation)\n",
        "    return rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPjhsA8krm0w"
      },
      "source": [
        "# 1. Steering (training)\n",
        "Use a pair of audio examples in order to construct neural audio effects.\n",
        "\n",
        "There are two options. Either start with the pre-loaded audio examples, or upload your own clean/processed audio recordings for the steering process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbLuQpbsb97"
      },
      "source": [
        "a.) Use some of our pre-loaded audio examples. Choose from the compressor or reverb effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HCeohtbwseO3"
      },
      "outputs": [],
      "source": [
        "#@title Use pre-loaded audio examples for steering\n",
        "effect_type = \"Amp\" #@param [\"Compressor\", \"Reverb\", \"UltraTab\", \"Amp\"]\n",
        "\n",
        "if effect_type == \"Compressor\":\n",
        "  input_file = \"audio/drum_kit_clean.wav\"\n",
        "  output_file = \"audio/drum_kit_comp_agg.wav\"\n",
        "elif effect_type == \"Reverb\":\n",
        "  input_file = \"audio/acgtr_clean.wav\"\n",
        "  output_file = \"audio/acgtr_reverb.wav\"\n",
        "elif effect_type == \"UltraTab\":\n",
        "  input_file = \"audio/acgtr_clean.wav\"\n",
        "  output_file = \"audio/acgtr_ultratab.wav\"\n",
        "elif effect_type == \"Amp\":\n",
        "  input_file = \"audio/ts9_test1_in_FP32.wav\"\n",
        "  output_file = \"audio/ts9_test1_out_FP32.wav\"\n",
        "\n",
        "x, sample_rate = torchaudio.load(input_file)\n",
        "y, sample_rate = torchaudio.load(output_file)\n",
        "\n",
        "x = x[0:1,:]\n",
        "y = y[0:1,:]\n",
        "\n",
        "print(\"x shape\", x.shape)\n",
        "print(f\"x = {x}\")\n",
        "print(\"y shape\", y.shape)\n",
        "print(f\"y = {y}\")\n",
        "\n",
        "print(\"input file\", x.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=x, rate=sample_rate))\n",
        "print(\"output file\", y.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=y, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLulasUas996"
      },
      "source": [
        "Now its time to generate the neural audio effect by training the TCN to emulate the input/output function from the target audio effect. Adjusting the parameters will enable you to tweak the optimization process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z06ODrBuB4WY"
      },
      "outputs": [],
      "source": [
        "#@title TCN model training parameters\n",
        "kernel_size = 13 #@param {type:\"slider\", min:3, max:32, step:1}\n",
        "n_blocks = 4 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "dilation_growth = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n_channels = 32 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "n_iters = 300 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "length = 508032 #@param {type:\"slider\", min:0, max:524288, step:1}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "# reshape the audio\n",
        "x_batch = x.view(1,1,-1)\n",
        "y_batch = y.view(1,1,-1)\n",
        "\n",
        "print(f\"x_batch shape: {x_batch.shape}\")\n",
        "print(f\"y_batch shape: {y_batch.shape}\")\n",
        "\n",
        "# build the model\n",
        "model = TCN(\n",
        "    n_inputs=1,\n",
        "    n_outputs=1,\n",
        "    kernel_size=kernel_size, \n",
        "    n_blocks=n_blocks, \n",
        "    dilation_growth=dilation_growth, \n",
        "    n_channels=n_channels)\n",
        "rf = model.compute_receptive_field()\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Parameters: {params*1e-3:0.3f} k\")\n",
        "print(f\"Receptive field: {rf} samples or {(rf/sample_rate)*1e3:0.1f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup loss function, optimizer, and scheduler\n",
        "loss_fn_mse = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "ms1 = int(n_iters * 0.8)\n",
        "ms2 = int(n_iters * 0.95)\n",
        "milestones = [ms1, ms2]\n",
        "print(\n",
        "    \"Learning rate schedule:\",\n",
        "    f\"1:{lr:0.2e} ->\",\n",
        "    f\"{ms1}:{lr*0.1:0.2e} ->\",\n",
        "    f\"{ms2}:{lr*0.01:0.2e}\",\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones,\n",
        "    gamma=0.1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "summary = torchinfo.summary(model, (1, 1, 228308), device=device)\n",
        "print(summary)\n",
        "\n",
        "print(\"The reduction in the first layer is from the kernel size. The reduction in the subsequent layers is from the dilation factor and kernel size.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# move tensors to GPU\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "\n",
        "start_idx = rf \n",
        "stop_idx = start_idx + length\n",
        "\n",
        "# the data is the same with every iteration\n",
        "x_crop = x_batch[...,start_idx-rf+1:stop_idx]\n",
        "y_crop = y_batch[...,start_idx:stop_idx]\n",
        "\n",
        "print(f\"x_crop = {x_crop.shape}\")\n",
        "print(f\"y_crop = {y_crop.shape}\")\n",
        "\n",
        "########## iteratively update the weights\n",
        "\n",
        "# this is only for the progress bar\n",
        "pbar = tqdm(range(n_iters))\n",
        "\n",
        "for n in pbar:\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  y_hat = model(x_crop)\n",
        "  loss = loss_fn_mse(y_hat, y_crop)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "  if (n+1) % 1 == 0:\n",
        "    pbar.set_description(f\" Loss: {loss.item()} | \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"models/\"+name+\"/\"+name+\".pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run predictions\n",
        "### 0. Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"models/\"+name+\"/\"+name+\".pth\", map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. On the test audio data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Prediction #################################################\n",
        "# Test the model on the testing data #############################\n",
        "\n",
        "# needed because in the train we crop the target\n",
        "x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_hat = model(x_pad)\n",
        "\n",
        "input = x_batch.view(-1).detach().cpu().numpy()[-y_hat.shape[-1]:]\n",
        "output = y_hat.view(-1).detach().cpu().numpy()\n",
        "target = y_batch.view(-1).detach().cpu().numpy()\n",
        "\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "# apply highpass to outpu to remove DC\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "output = scipy.signal.sosfilt(sos, output)\n",
        "\n",
        "input /= np.max(np.abs(input))\n",
        "output /= np.max(np.abs(output))\n",
        "target /= np.max(np.abs(target))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(target, sr=sample_rate, color='b', alpha=0.5, ax=ax, label='Target')\n",
        "librosa.display.waveshow(output, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "\n",
        "print(\"Input (clean)\")\n",
        "IPython.display.display(IPython.display.Audio(data=input, rate=sample_rate))\n",
        "print(\"Target\")\n",
        "IPython.display.display(IPython.display.Audio(data=target, rate=sample_rate))\n",
        "print(\"Output\")\n",
        "IPython.display.display(IPython.display.Audio(data=output, rate=sample_rate))\n",
        "plt.legend()\n",
        "plt.show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuC70b_j28K5"
      },
      "outputs": [],
      "source": [
        "# Load and Preprocess Data ###########################################\n",
        "x_whole, sample_rate = torchaudio.load(\"audio/piano_clean.wav\")\n",
        "x_whole = x_whole[0,:]\n",
        "x_whole = x_whole.view(1,1,-1).to(device)\n",
        "\n",
        "# Padding on both sides of the receptive field\n",
        "x_whole = torch.nn.functional.pad(x_whole, (rf-1, rf-1))\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_whole = model(x_whole)\n",
        "\n",
        "x_whole = x_whole[..., -y_whole.shape[-1]:]\n",
        "\n",
        "y_whole /= y_whole.abs().max()\n",
        "\n",
        "# apply high pass filter to remove DC\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "y_whole = scipy.signal.sosfilt(sos, y_whole.cpu().view(-1).numpy())\n",
        "\n",
        "x_whole = x_whole.view(-1).cpu().numpy()\n",
        "\n",
        "y_whole = (y_whole * 0.8)\n",
        "IPython.display.display(IPython.display.Audio(data=x_whole, rate=sample_rate))\n",
        "IPython.display.display(IPython.display.Audio(data=y_whole, rate=sample_rate))\n",
        "\n",
        "x_whole /= np.max(np.abs(x_whole))\n",
        "y_whole /= np.max(np.abs(y_whole))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(y_whole, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "librosa.display.waveshow(x_whole, sr=sample_rate, alpha=0.5, ax=ax, label='Input', color=\"blue\")\n",
        "plt.legend()\n",
        "plt.show(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. On a number sequence (to control inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model simple number sequence to compare with inference #\n",
        "X_testing_2 = np.array([])\n",
        "\n",
        "for i in range(0, 2048+rf-1):\n",
        "    X_testing_2 = np.append(X_testing_2, i*0.000001)\n",
        "\n",
        "X_testing_2 = torch.from_numpy(X_testing_2).view(1,1,-1)\n",
        "\n",
        "print(\"Running prediction..\")\n",
        "prediction_2 = model(X_testing_2.to(device).float())\n",
        "\n",
        "print(f\"prediction {prediction_2}\")\n",
        "\n",
        "print(\"X_testing_2 shape: \", X_testing_2.shape)\n",
        "print(\"prediction_2 shape: \", prediction_2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export as pt and onnx model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the model to TorchScript ##################################\n",
        "scripted_module = torch.jit.script(model)\n",
        "scripted_module.save(\"models/\"+name+\"/\"+\"steerable-nafx-dynamic.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the model to ONNX ##########################################\n",
        "model_input_size = rf\n",
        "\n",
        "# An example input tensor with a placeholder size\n",
        "example_x = torch.rand(1, 1, model_input_size).to(device)\n",
        "\n",
        "# Filepath for the exported model\n",
        "filepath = f\"models/{name}/steerable-nafx-libtorch-dynamic.onnx\"\n",
        "\n",
        "# Export the model to ONNX with a dynamic input size\n",
        "torch.onnx.export(model=model,\n",
        "                  args=example_x,\n",
        "                  f=filepath,\n",
        "                  export_params=True,\n",
        "                  opset_version=17,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names=['input'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes={'input': {2: 'model_input_size'}, 'output': {2: 'model_output_size'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train also a model with only 3 TCN blocks for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title TCN model training parameters\n",
        "kernel_size = 13 #@param {type:\"slider\", min:3, max:32, step:1}\n",
        "n_blocks = 3 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "dilation_growth = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n_channels = 32 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "n_iters = 10 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "length = 508032 #@param {type:\"slider\", min:0, max:524288, step:1}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "# reshape the audio\n",
        "x_batch = x.view(1,1,-1)\n",
        "y_batch = y.view(1,1,-1)\n",
        "\n",
        "print(f\"x_batch shape: {x_batch.shape}\")\n",
        "print(f\"y_batch shape: {y_batch.shape}\")\n",
        "\n",
        "# build the model\n",
        "model_3_blocks = TCN(\n",
        "    n_inputs=1,\n",
        "    n_outputs=1,\n",
        "    kernel_size=kernel_size, \n",
        "    n_blocks=n_blocks, \n",
        "    dilation_growth=dilation_growth, \n",
        "    n_channels=n_channels)\n",
        "rf = model_3_blocks.compute_receptive_field()\n",
        "params = sum(p.numel() for p in model_3_blocks.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Parameters: {params*1e-3:0.3f} k\")\n",
        "print(f\"Receptive field: {rf} samples or {(rf/sample_rate)*1e3:0.1f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup loss function, optimizer, and scheduler\n",
        "loss_fn_mse = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model_3_blocks.parameters(), lr)\n",
        "ms1 = int(n_iters * 0.8)\n",
        "ms2 = int(n_iters * 0.95)\n",
        "milestones = [ms1, ms2]\n",
        "print(\n",
        "    \"Learning rate schedule:\",\n",
        "    f\"1:{lr:0.2e} ->\",\n",
        "    f\"{ms1}:{lr*0.1:0.2e} ->\",\n",
        "    f\"{ms2}:{lr*0.01:0.2e}\",\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones,\n",
        "    gamma=0.1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "summary = torchinfo.summary(model_3_blocks, (1, 1, 228308), device=device)\n",
        "print(summary)\n",
        "\n",
        "print(\"The reduction in the first layer is from the kernel size. The reduction in the subsequent layers is from the dilation factor and kernel size.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# move tensors to GPU\n",
        "if torch.cuda.is_available():\n",
        "  model_3_blocks.to(device)\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "\n",
        "start_idx = rf \n",
        "stop_idx = start_idx + length\n",
        "\n",
        "# the data is the same with every iteration\n",
        "x_crop = x_batch[...,start_idx-rf+1:stop_idx]\n",
        "y_crop = y_batch[...,start_idx:stop_idx]\n",
        "\n",
        "print(f\"x_crop = {x_crop.shape}\")\n",
        "print(f\"y_crop = {y_crop.shape}\")\n",
        "\n",
        "########## iteratively update the weights\n",
        "\n",
        "# this is only for the progress bar\n",
        "pbar = tqdm(range(n_iters))\n",
        "\n",
        "for n in pbar:\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  y_hat = model_3_blocks(x_crop)\n",
        "  loss = loss_fn_mse(y_hat, y_crop)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "  if (n+1) % 1 == 0:\n",
        "    pbar.set_description(f\" Loss: {loss.item()} | \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. On the test audio data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Prediction #################################################\n",
        "# Test the model on the testing data #############################\n",
        "\n",
        "# needed because in the train we crop the target\n",
        "x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "\n",
        "model_3_blocks.eval()\n",
        "with torch.no_grad():\n",
        "  y_hat = model_3_blocks(x_pad)\n",
        "\n",
        "input = x_batch.view(-1).detach().cpu().numpy()[-y_hat.shape[-1]:]\n",
        "output = y_hat.view(-1).detach().cpu().numpy()\n",
        "target = y_batch.view(-1).detach().cpu().numpy()\n",
        "\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "# apply highpass to outpu to remove DC\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "output = scipy.signal.sosfilt(sos, output)\n",
        "\n",
        "input /= np.max(np.abs(input))\n",
        "output /= np.max(np.abs(output))\n",
        "target /= np.max(np.abs(target))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(target, sr=sample_rate, color='b', alpha=0.5, ax=ax, label='Target')\n",
        "librosa.display.waveshow(output, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "\n",
        "print(\"Input (clean)\")\n",
        "IPython.display.display(IPython.display.Audio(data=input, rate=sample_rate))\n",
        "print(\"Target\")\n",
        "IPython.display.display(IPython.display.Audio(data=target, rate=sample_rate))\n",
        "print(\"Output\")\n",
        "IPython.display.display(IPython.display.Audio(data=output, rate=sample_rate))\n",
        "plt.legend()\n",
        "plt.show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Preprocess Data ###########################################\n",
        "x_whole, sample_rate = torchaudio.load(\"audio/piano_clean.wav\")\n",
        "x_whole = x_whole[0,:]\n",
        "x_whole = x_whole.view(1,1,-1).to(device)\n",
        "\n",
        "# Padding on both sides of the receptive field\n",
        "x_whole = torch.nn.functional.pad(x_whole, (rf-1, rf-1))\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_whole = model_3_blocks(x_whole)\n",
        "\n",
        "x_whole = x_whole[..., -y_whole.shape[-1]:]\n",
        "\n",
        "y_whole /= y_whole.abs().max()\n",
        "\n",
        "# apply high pass filter to remove DC\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "y_whole = scipy.signal.sosfilt(sos, y_whole.cpu().view(-1).numpy())\n",
        "\n",
        "x_whole = x_whole.view(-1).cpu().numpy()\n",
        "\n",
        "y_whole = (y_whole * 0.8)\n",
        "IPython.display.display(IPython.display.Audio(data=x_whole, rate=sample_rate))\n",
        "IPython.display.display(IPython.display.Audio(data=y_whole, rate=sample_rate))\n",
        "\n",
        "x_whole /= np.max(np.abs(x_whole))\n",
        "y_whole /= np.max(np.abs(y_whole))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(y_whole, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "librosa.display.waveshow(x_whole, sr=sample_rate, alpha=0.5, ax=ax, label='Input', color=\"blue\")\n",
        "plt.legend()\n",
        "plt.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export as pt and onnx model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the model to TorchScript ##################################\n",
        "scripted_module = torch.jit.script(model_3_blocks)\n",
        "scripted_module.save(\"models/\"+name+\"/\"+\"steerable-nafx-3_blocks-dynamic.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the model to ONNX ##########################################\n",
        "model_input_size = rf\n",
        "\n",
        "# An example input tensor with a placeholder size\n",
        "example_x = torch.rand(1, 1, model_input_size).to(device)\n",
        "\n",
        "# Filepath for the exported model\n",
        "filepath = f\"models/{name}/steerable-nafx-3_blocks-libtorch-dynamic.onnx\"\n",
        "\n",
        "# Export the model to ONNX with a dynamic input size\n",
        "torch.onnx.export(model=model_3_blocks,\n",
        "                  args=example_x,\n",
        "                  f=filepath,\n",
        "                  export_params=True,\n",
        "                  opset_version=17,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names=['input'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes={'input': {2: 'model_input_size'}, 'output': {2: 'model_output_size'}})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "A6x_gvWLIC8c",
        "SqxbdaQv7DD3",
        "UKmzWBaSJEcw"
      ],
      "name": "Steerable discovery of neural audio effects.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
